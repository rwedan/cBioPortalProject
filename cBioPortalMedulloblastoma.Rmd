---
title: "cBioPortal: Medulloblastoma"
author: "Tim Triche, Jr."
date: "12/8/2021"
output:
  html_document:
    keep_md: true
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Automating cBioPortal queries

## An unpackaged Rmarkdown example

This document pulls some data from [cBioPortal](https://cbioportal.org/) via `cgdsr`,
and performs a bit of analysis on it.  (A separate example exists for slides,
as with a 5-minute presentation.) Let's begin by looking at medulloblastoma,
a mostly-pediatric tumor of the medulla oblongata, or lower brainstem. We will 
pull a small study from [cBioPortal](https://cBioPortal.org) and then see how the
estimate of co-mutation odds for two genes hold up in a much larger study. 

## Medulloblastoma background information

[A recent paper from Volker Hovestadt](https://www.nature.com/articles/s41586-019-1434-6) 
provides some more details on the features of these tumors, which have (like most
childhood brain cancers) been an object of intense study (including at VAI). Of 
note, Hedgehog signaling, Wnt signaling, and a constellation of alterations lumped
together via DNA methylation profiling as "Group 3" and "Group 4" define the WHO 
subtypes of medulloblastoma among patients thus far characterized.

## cBioPortal data via the `cgdsr` package 

Let's pull some data from cBioPortal using the [cgdsr](https://cran.r-project.org/web/packages/cgdsr/index.html) package and
see what studies are available for this disease.

The first step is to  create a CGDS (Cancer Genome Data Server)
object to manage cBioPortal queries.

<details>
  <summary>Load required libraries</summary>
```{r, loadLibraries}
install.packages("cgdsr")
install.packages("kableExtra")
install.packages("pheatmap")
install.packages("janitor")

library(cgdsr)
library(tidyverse)
library(kableExtra)
library(pheatmap)
library(janitor)

```
</details>

```{r, cgdsr}

mycgds <- CGDS("http://www.cbioportal.org/")
show(mycgds)

```

That's not terribly useful. Let's ask the cBioPortal object what studies we can query. 

```{r, studies}

studies <- getCancerStudies(mycgds)
glimpse(studies)

```

Much better.  It looks like there are 333 studies to choose from at the moment.

###############################################################################
###############################################################################

Starting my notes & changes -RW

## Fetching mesothelioma studies 

Let's narrow our field down to the ones that involve mesothelioma.

```{r, medulloStudies}

getCancerStudies(mycgds) %>% 
  filter(str_detect(name, "Mesothelioma")) %>% 
  select(cancer_study_id, name) %>% 
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))

```

Finding the Sample Size of these two studies... by reporting numbers in the description column

```{r, studySizes}

getCancerStudies(mycgds) %>% 
  filter(str_detect(name, "Mesothelioma")) %>% 
  mutate(n = as.integer(str_extract(description, "[0-9]+"))) %>% 
  select(cancer_study_id, n, name) %>% 
  arrange(n) %>% 
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))

```

The 2016 value is a bit suspect to me.. now will be attempting to find n

```{r}
Mesothelioma <- subset(studies, cancer_study_id == "meso_tcga_pan_can_atlas_2018" | cancer_study_id == "meso_tcga"| cancer_study_id == "plmeso_nyu_2015")

print(Mesothelioma$description)
```

This also was not helpful! Just went straight to the website cbioportal.org to find these three studies
```{r}

```

meso_tcga, n=87
meso_tcga_pan_can_atlas_2018, n=87
plmeso_nyu_2015, n=22

"meso_tcga" and "meso_tcga_pan_can_atlas_2018" are likely the same samples?


## Fetching case lists to collate mutations and other aberrations 


Starting with the meso_tcga study! n=87

```{r, getMutationData}

meso_tcga <- "meso_tcga"

# the IDs of the cases in the PCGP MBL study
getCaseLists(mycgds, cancerStudy="meso_tcga") %>% 
  filter(case_list_name == "Samples with mutation data") ->
    tcga_caselists


# grab the list of lesions
getGeneticProfiles(mycgds, meso_tcga) %>% 
  filter(genetic_profile_name == "Mutations") %>% 
  pull(genetic_profile_id) ->
    tcga_mutations_profile

# a few "greatest hits" mutations seen in Mesothelioma, as per google
meso_genes <- c("BAP1", "NF2", "CDKN2A", "CDKN2B", "TP53")

# get the mutations data and tidy it up
get_muts <- function(x, genes, ...) {
  
  muts <- getProfileData(x, genes, ...)
  is.na(muts) <- (muts == "NaN")
  muts[is.na(muts)] <- 0
  muts[muts != 0] <- 1  
  rn <- rownames(muts)
  muts <- data.frame(lapply(muts, as.integer))
  rownames(muts) <- rn
  return(muts[, genes])
  
}

# We throw out the nature of the mutation here, which is rarely a wise idea (if ever)
muts <- get_muts(mycgds, 
                 meso_genes, 
                 geneticProfiles=tcga_mutations_profile, 
                 caseList=tcga_caselists$case_list_id)

muts %>% 
  filter(rowSums(.) > 0) %>%
  t() %>%
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))

```


## Plotting (some of) the results from our query


```{r, oncoprint, echo=FALSE}

# the cgdsr plot function sucks
# let's use pheatmap instead
pheatmap(t(data.matrix(muts)), col=c("white", "darkred"), cluster_rows=FALSE,
         clustering_distance_cols="manhattan", clustering_method="ward.D2", legend=FALSE)

```
Looks like there is a group of TP53 mutant and TP53 non-mutants, no CDKN2A or CDKN2B mutants, and a mix of
BAP1 and NF2 mutants in the mix.


Are _BAP1_ and _NF2_ comutated? 

```{r, CTNNB1_and_DDX3X}

message("Chi-squared p-value:", appendLF = FALSE)
muts %>% 
  tabyl(BAP1, NF2) %>% 
  chisq.test(simulate.p.value = TRUE) %>% 
  getElement("p.value")

message("Fisher's exact test p-value:", appendLF = FALSE)
muts %>% 
  tabyl(BAP1, NF2) %>% 
  fisher.test(simulate.p.value = TRUE) %>% 
  getElement("p.value")


```
p = 0.5128434, this is definitely not significant. 


___________________________________
# Power, sample size, and variance

## More is not better, better is better...
What about the smaller sample size study from NYU! n=22

```{r, Wnt_and_Ddx_revisited}

meso_NYU <- "plmeso_nyu_2015"

getCaseLists(mycgds,  meso_NYU) %>% 
  filter(case_list_name == "Samples with mutation data") ->
   NYU_caselists 

# grab the list of lesions
getGeneticProfiles(mycgds, meso_NYU) %>% 
  filter(genetic_profile_name == "Mutations") %>% 
  pull(genetic_profile_id) ->
    NYU_mutations_profile

# a few "greatest hits" mutations seen in Mesothelioma, as per google
#This study required me to narrow down the number of genes in order to run
meso_genes <- c("BAP1", "NF2", "TP53")

# grab the mutation matrix for the genes as before
NYU_muts <- get_muts(mycgds, meso_genes,
                           NYU_mutations_profile, 
                           NYU_caselists$case_list_id)

# out of curiosity, how many of each mutation do we see here? 
colSums(NYU_muts)
```

The third study is the same as the TCGA one so I did not do the same analysis! 

This is the code that I would have run for the NYU study...
```{r, replicatingResults}
message("Chi-squared p-value (NYU):", appendLF = FALSE)
NYU_muts %>% 
  tabyl(BAP1, NF2) %>% 
  chisq.test(simulate.p.value = TRUE) %>% 
  getElement("p.value")

message("Fisher's exact test p-value (NYU):", appendLF = FALSE)
NYU_muts %>% 
  tabyl(BAP1, NF2) %>% 
  fisher.test(simulate.p.value = TRUE) %>% 
  getElement("p.value")

```
Both, once again, are non significant with a p value of around 0.5.

What about other gene associations?
```{r}
message("Chi-squared p-value (NYU):", appendLF = FALSE)
NYU_muts %>% 
  tabyl(BAP1, TP53) %>% 
  chisq.test(simulate.p.value = TRUE) %>% 
  getElement("p.value")

message("Fisher's exact test p-value (NYU):", appendLF = FALSE)
NYU_muts %>% 
  tabyl(BAP1, TP53) %>% 
  fisher.test(simulate.p.value = TRUE) %>% 
  getElement("p.value")

message("Chi-squared p-value (NYU):", appendLF = FALSE)
NYU_muts %>% 
  tabyl(NF2, TP53) %>% 
  chisq.test(simulate.p.value = TRUE) %>% 
  getElement("p.value")

message("Fisher's exact test p-value (NYU):", appendLF = FALSE)
NYU_muts %>% 
  tabyl(NF2, TP53) %>% 
  fisher.test(simulate.p.value = TRUE) %>% 
  getElement("p.value")


```
There are no significant associations, with similar p values to the larger sample size cohort.

There were significant associations between TP53 and CDKN2A/2B, but all of the CDKN2A/2B


```{r, anotherPlot}

# let's use pheatmap again
pheatmap(t(data.matrix(NYU_muts)), col=c("white", "darkred"), cluster_rows=FALSE,
         clustering_distance_cols="manhattan", clustering_method="ward.D2", legend=FALSE)

```

Are these the same study?

```{r, doubleDipping}

tcga_caselists %>% 
  pull(case_ids) %>% 
  str_split(pattern=" ") %>% 
  getElement(1) -> 
    tcga_cases

pan_caselists %>% 
  pull(case_ids) %>% 
  str_split(pattern=" ") %>% 
  getElement(1) -> 
    pan_cases

NYU_caselists %>%
  pull(case_ids) %>% 
  str_split(pattern=" ") %>% 
  getElement(1) ->
    NYU_cases

intersect(NYU_cases, tcga_cases) # no cases found here
intersect(tcga_cases, pan_cases) # all of these cases are the same

```
The "pan" and tgca cases are all the same except for one patient sample.

```{r, dropCases}

tcga_only <- setdiff(tcga_cases, pan_cases)

```



```{r, significance}



```

to look at odds ratios:

```{r, oddsRatioReplication}

muts %>% 
  tabyl(BAP1, NF2) %>% 
  fisher.test() %>% 
  getElement("estimate") -> 
    tcga_estimate

NYU_muts %>% 
  tabyl(BAP1, NF2) %>% 
  fisher.test() %>% 
  getElement("estimate") ->
    NYU_estimate

message("Co-occurrence odds ratio (NYU cases): ", round(NYU_estimate, 3))
message("Co-occurrence odds ratio (TCGA cases): ", round(tcga_estimate, 3))
message("Effect size inflation, NYU vs. TCGA: ",
        round(NYU_estimate / tgca_estimate), "x")
        

```
Co-occurrence odds ratio (NYU cases): 4.029 (n=22)
Co-occurrence odds ratio (TCGA cases): 1.656 (n=87)


The results replicated, but the odds ratio is still 4x!


##############################################################################################
This is the end of my edits for Mesothelioma studies
##############################################################################################


# The winner's curse and replication 

It turns out this happens a lot. It's rarely intentional (see above).
Early results in small studies can over- or under-estimate effect sizes
(and, sometimes, significance or sign) relative to larger or later studies.
(Unfortunately, the later studies almost invariably do not make the news.)

Recent work has quantified [the many challenges of replicating scientific research](https://elifesciences.org/articles/67995), particularly
[the difficulty of interpreting the results](https://elifesciences.org/articles/71601)
when all is said and done. In short, just over a quarter of high-profile cancer
biology studies that the authors tried to replicate could be started at all. Of 
those that could be reproduced, about half replicated. Already in this class, we've
seen at least one result that _could not possibly have been interpreted as significant_ (because it was literal experimental noise) cited over 1000 times as justification for continuing to parrot the same nonsense. Usually the problems are a bit more subtle, and sometimes they're just bad luck. More specifically, regression to the mean.

## Regression to the mean 

[Regression to the mean](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) 
describes the tendency of successive estimates, particularly from small samples, 
to over- and under-shoot the true effect sizes. This cuts both ways: usually in
biology and medicine we worry about overestimates of effect sizes, but smallish 
experiments can also underestimate important effects. Depending on incentives,
one or the other may be more desirable than a consistent estimate of modest size.

This is not limited to experimental biology; it can be readily seen in 
[the gold standard in biomedical research, the randomized clinical trial](https://www.bmj.com/content/346/bmj.f2304). In short, if you run under-
powered experiments, most of the time you'll miss an effect even if it's there, 
but sometimes you'll wildly overestimate. Neither of these are good things.


# Simulations, power, and reducing the variance of estimates 

Let's make this concrete with some simulations.  We'll adjust the effect size 
slightly for co-mutations of _CTNNB1_ and _DDX3X_ in medulloblastoma, then run
some simulations at various sample sizes to see what we see. (You can also use
an analytical estimate via `power.prop.test` and similar, but for better or worse,
simulating from a noisy generating process is about the same amount of work for 
powering Fisher's test, as is the case for many tests of significance, as in trials.)

In order to take into account uncertainty (proposing that we found what we found
in the original PCGP study and wanted to estimate the odds of seeing it again), 
we'll use the [beta distribution](http://varianceexplained.org/statistics/beta_distribution_and_baseball/) to capture a "noisy" estimate of a proportion.  Specifically, let's
use the original mutation table to estimate each from the PCGP data. 

```{r, probabilities}

neither <- nrow(subset(muts, CTNNB1 == 0 & DDX3X == 0))
CTNNB1 <- nrow(subset(muts, CTNNB1 == 1 & DDX3X == 0))
DDX3X <- nrow(subset(muts, CTNNB1 == 0 & DDX3X == 1))
both <- nrow(subset(muts, CTNNB1 == 1 & DDX3X == 1))

```

Now we have all we really need to simulate. Formally, we will model it like so.

* For each sample, we simulate the occurrence of _one_ mutation.
* If a sample has _one_ mutation, we simulate which one (CTNNB1 or DDX3X).
* If a sample has fewer or more than _one_ mutation, we simulate which.

We can do this repeatedly to estimate the distribution of test
statistics to expect if we run this experiment quite a few times,
with both smaller and larger total sample sizes. We're assuming that
the dependency structure is fairly stable (is this reasonable?). 

The `Beta(a, b)` distribution above is continuous between 0 and 1, and its shape depends
on the values of `a` and `b`. For example, we can plot each of the above using the 
St. Jude's-derived values to get a feel for how "mushy" our guesses are given
the number of samples in the St Jude PCGP study. Effectively, we propagate our 
underlying uncertainty about parameters by drawing them from a sensible generator,
and that sensible generator is a beta distribution reflecting our sample size. 

```{r, betas}

a <- CTNNB1
b <- DDX3X 

p_one <- function(x) dbeta(x, (a + b), (both + neither))
p_both <- function(x) dbeta(x, both, (a + b + neither))
p_both_if_not_one <- function(x) dbeta(x,  both, neither)

plot(p_one, main="Pr(A|B & !(A & B))")
plot(p_both, main="Pr(A & B)")
plot(p_both_if_not_one, main="Pr( (A & B) | (A + B != 1))")

```

Now for `n` samples, we can simulate appropriately "noisy" 2x2 tables with that many subjects.

```{r, simulate2x2}

sim2x2 <- function(n, neither, a, b, both) {

  p_one <- rbeta(1, (a + b), (both + neither))
  p_both <- rbeta(1, both, neither)
  p_a <- rbeta(1, a, b)
  
  n_a_b <- rbinom(1, n, p_one)
  n_neither_both <- n - n_a_b
  n_both <- rbinom(1, n_neither_both, p_both)
  n_neither <- n_neither_both - n_both
  n_a <- rbinom(1, n_a_b, p_a)
  n_b <- n_a_b - n_a

  as.table(matrix(c(n_neither, n_a, n_b, n_both), nrow=2))
  
}

```

Let's give it a shot.

```{r, someSims}

a <- CTNNB1
b <- DDX3X 

sim2x2(n=nrow(muts), neither, a, b, both)
```

That seems to work fine (we could more directly simulate the odds of neither+either and a+b, feel free to implement that instead). If you want an analytical estimate for an asymptotic 
test (`prop.test`), R also provides that, but beware: it doesn't really take into account 
sampling variance (i.e. uncertainty about the parameter estimates). Let's do that ourselves.

```{r, wrappers}

# fairly generic 
simFisher <- function(n, neither, a, b, both) fisher.test(sim2x2(n, neither, a, b, both))

# using the values we've already set up to simulate from: 
simFetP <- function(n) simFisher(n, neither, a, b, both)$p.value

```

The `replicate` function is quite helpful here. Let's suppose the St. Jude 
study is representative of medulloblastoma generally. We'll simulate 1000 
studies of sizes between 10 and 500 to see how often our (true!) difference
in proportions registers as significant at p < 0.05. 

```{r, sampleSizes}

powerN <- function(n, alpha=0.05) {
  res <- table(replicate(1000, simFetP(n=n)) < alpha)
  res["TRUE"] / sum(res)
}

for (N in c(10, 30, 50, 100, 300, 500)) {
  message("Power at alpha = 0.05 with n = ", N, ": ", powerN(N) * 100, "%")
}

```
_Question_: What's the estimated power with a sample size of 37? 

<details>
  <summary>Click here for an answer</summary>
```{r, power37}
   message("Power at alpha = 0.05 with n = ", 37, ": ", powerN(37) * 100, "%")
```
</details>

_Question_: How does that compare to `power.prop.test` with p1 = (neither+both)/(all),
p2 = (CTNNB1only + DDX3Xonly)/(all), and n=37? Does this seem like and over- or under-
estimate relative to Fisher's exact test (above)?

<details>
  <summary>Click here for an answer</summary>
```{r, powerPropTest}
  power.prop.test(n=37, p1=(35/37), p2=(2/37))
```
</details>

For reasons we'll see shortly, I find the above to be a gross overestimate. 
What about our odds ratio estimates? Do they hop around all over the place?
Let's add a pseudocount to the shrunken odds ratio estimator to stabilize them.

```{r, oddsRatios}


# how wild are our odds ratios at a given N?
shrinkOR <- function(n, pseudo=2) {
  res <- sim2x2(n, neither, a, b, both) + pseudo
  odds <- (res[1,1] * res[2,2]) / (res[1,2] * res[2,1])
  return(odds)
}

OR0 <- function(n) replicate(1000, shrinkOR(n, pseudo=1e-6))

for (N in c(10, 20, 40, 80)) {
  hist(OR0(n=N), xlab="Estimate", main=paste("Near-raw odds ratio distribution with N =", N))
}

# And if we shrink a bit (i.e., apply a prior)?
ORs <- function(n) replicate(1000, shrinkOR(n))

for (N in c(10, 20, 40, 80)) {
  hist(ORs(n=N), xlab="Estimate", main=paste("Shrunken odds ratio distribution with N =", N))
}
  
```

(Some of you may already know the log-odds ratio trick to turn the above into a bell curve.
If not, try re-plotting the histograms above, but using the log of the OR estimates.)
```{r}

#Re-plotting wiht log of OR estimates

#how wild are our odds ratios at a given N?
shrinkORlog <- function(n, pseudo=2) {
  res <- sim2x2(n, neither, a, b, both) + pseudo
  odds <- log10((res[1,1] * res[2,2]) / (res[1,2] * res[2,1]))
  return(odds)
}

# how wild are our odds ratios at a given N?
shrinkOR <- function(n, pseudo=2) {
  res <- sim2x2(n, neither, a, b, both) + pseudo
  odds <- (res[1,1] * res[2,2]) / (res[1,2] * res[2,1])
  return(odds)
}

OR0log <-function(n) replicate(1000, shrinkOR(n, pseudo=1e-6))

for (N in c(10, 20, 40, 80)) {
  hist(OR0log(n=N), xlab="Estimate", main=paste("Near-raw odds ratio distribution with N =", N))
}

# And if we shrink a bit (i.e., apply a prior)?
ORslog <- function(n) replicate(1000, shrinkORlog(n))

for (N in c(10, 20, 40, 80)) {
  hist(ORslog(n=N), xlab="Estimate", main=paste("Shrunken odds ratio distribution with N =", N))
}
  



```

The tails of the estimates are pretty long, but the mass concentrates quickly near the true
parameter estimate. (This is also why resampling approaches can help stabilize estimates:
if you have enough data to estimate a parameter, resampling can also estimate how fragile 
your estimates are, and therefore how trustworthy they are. That's why we bootstrap.) 


# Thoughts and questions

* Run `with(muts, table(CTNNB1, DDX3X)) %>% fisher.test`.  What does the 95% CI represent?
```{r}
with(muts, table(CTNNB1, DDX3X)) %>% fisher.test
```


* Fisher's exact test is a special case of hypergeometric testing. What others exist? 
What are these tests typically used for, and what assumptions are made regarding categories?

Barnard's double-binomial test. For 2x2 contingency tables
Boschloo's hypergeometric text. For 2x2 contingency tables, uses Fisher as test statistic


* What happens if you test multiple hypotheses, or apply multiple tests of the same?

* Does it matter which correction method in `p.adjust` you use to correct if you do?

* Is there a situation in cancer genetic epidemiology where the FWER would make more 
sense than the FDR (i.e., where you want to bound the probability of any false positives)?

* Can you rig up a simulation where the cost for a false negative is much greater than for 
a false positive, and tally up the results of various `p.adjust` methods in this situation?

* Can you rig up a simulation where the cost for a false positive is much greater than for 
a false negative, and tally up the results of various `p.adjust` schemes for that? 

* (Nontrivial) How would you adjust this for a 2x3 or 3x2 table like in Wang et al. 2014?

* (Nontrivial) How does the addition of a pseudocount stabilize the variance of estimates? What is being traded away when we do this, and is it (typically) a worthwhile trade?
